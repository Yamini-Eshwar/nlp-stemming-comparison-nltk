{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1f415c-9750-4cf7-856f-c5c355dcabb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vamsi\\anaconda3\\envs\\nlp_env\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\vamsi\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\vamsi\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vamsi\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vamsi\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vamsi\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89fded1f-2efa-4140-a2a7-817594cf3927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vamsi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# print(\"NLTK is working!\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "AI = \"\"\"Hi John,\n",
    "\n",
    "Hope you're doing well. I wanted to check in on the proposal.\n",
    "\n",
    "Let me know if you're free for a quick call today.\n",
    "\n",
    "Thanks,\n",
    "Alice\"\"\"\n",
    "\n",
    "# word_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe627a03-a80f-45d0-8ba8-5e288fffa9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hi John,\\n\\nHope you're doing well.\",\n",
       " 'I wanted to check in on the proposal.',\n",
       " \"Let me know if you're free for a quick call today.\",\n",
       " 'Thanks,\\nAlice']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35573ef4-2c4f-4266-bceb-4e4c43cc2fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi John,',\n",
       " \"Hope you're doing well. I wanted to check in on the proposal.\",\n",
       " \"Let me know if you're free for a quick call today.\",\n",
       " 'Thanks,\\nAlice']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize # splits at blank lines \\n\\n\n",
    "blankline_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf532a04-712a-4f1f-9b4d-ac2f00bdfe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'John,',\n",
       " 'Hope',\n",
       " \"you're\",\n",
       " 'doing',\n",
       " 'well.',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'check',\n",
       " 'in',\n",
       " 'on',\n",
       " 'the',\n",
       " 'proposal.',\n",
       " 'Let',\n",
       " 'me',\n",
       " 'know',\n",
       " 'if',\n",
       " \"you're\",\n",
       " 'free',\n",
       " 'for',\n",
       " 'a',\n",
       " 'quick',\n",
       " 'call',\n",
       " 'today.',\n",
       " 'Thanks,',\n",
       " 'Alice']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer # word tokenizer returns words and punctuation in a seperate list \n",
    "WhitespaceTokenizer().tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1792e6b-e64e-4742-aba6-9d78210b2da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Good apple cost Rs.3.38 in hyderabad. PLease buy two of them. okay.'\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize # strictly splits on words and punctuation, tokens increases\n",
    "w_p = wordpunct_tokenize(s)\n",
    "len(w_p) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6f60b-60b2-457a-8cab-58c229ca166f",
   "metadata": {},
   "source": [
    "### Tokenization is divided into 3 parts \n",
    ". ngrams = tokens with >3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ef35596-68d0-406e-9d56-9dbe8b1c82cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'the', 'best', 'and', 'most'),\n",
       " ('the', 'best', 'and', 'most', 'beautiful'),\n",
       " ('best', 'and', 'most', 'beautiful', 'thing'),\n",
       " ('and', 'most', 'beautiful', 'thing', 'in'),\n",
       " ('most', 'beautiful', 'thing', 'in', 'the'),\n",
       " ('beautiful', 'thing', 'in', 'the', 'world'),\n",
       " ('thing', 'in', 'the', 'world', 'can'),\n",
       " ('in', 'the', 'world', 'can', 'not'),\n",
       " ('the', 'world', 'can', 'not', 'be'),\n",
       " ('world', 'can', 'not', 'be', 'seen'),\n",
       " ('can', 'not', 'be', 'seen', 'or'),\n",
       " ('not', 'be', 'seen', 'or', 'even'),\n",
       " ('be', 'seen', 'or', 'even', 'touched')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "string = 'hello the best and most beautiful thing in the world cannot be seen or even touched'\n",
    "\n",
    "quote_tokens = nltk.word_tokenize(string)\n",
    "len(quote_tokens)\n",
    "\n",
    "quote_bigrams = list(nltk.bigrams(quote_tokens))\n",
    "len(quote_bigrams)\n",
    "\n",
    "quote_trigrams = list(nltk.trigrams(quote_tokens))\n",
    "len(quote_trigrams)\n",
    "\n",
    "quote_ngrams = list(nltk.ngrams(quote_tokens, 5))\n",
    "quote_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9e0789c-9191-45f2-81fc-9e5376ca89e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "given : given\n",
      "gave : gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "\n",
    "pst.stem('affection')\n",
    "pst.stem('playing')\n",
    "pst.stem('maximum')\n",
    "\n",
    "words_to_stem = ['give', 'giving', 'given', 'gave']\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words + ' : ' + pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5862ff44-93e4-4c97-8cb7-d3cde395087f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4),\n",
       " (5, 12),\n",
       " (13, 17),\n",
       " (18, 19),\n",
       " (19, 23),\n",
       " (24, 26),\n",
       " (27, 30),\n",
       " (31, 32),\n",
       " (32, 36),\n",
       " (36, 37),\n",
       " (37, 38),\n",
       " (39, 45),\n",
       " (46, 47),\n",
       " (47, 50),\n",
       " (50, 51),\n",
       " (52, 54),\n",
       " (56, 59),\n",
       " (60, 62),\n",
       " (63, 68),\n",
       " (69, 75)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''Good muffins cost $3.38\\nin New (York). Please (buy) me\\n two of them.\\nThanks '''\n",
    "\n",
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "\n",
    "list(NLTKWordTokenizer().span_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24991b6b-58c0-4677-b478-c45b37e3c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Love Pizza.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer # join fails badly at punctions \n",
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "detok.detokenize(['I', 'Love', 'Pizza', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2dc22668-9751-48ec-9eb9-cd458c1a33df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'paid', '$3.30', 'for', '2', 'apples']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "text = 'I paid $3.30 for 2 apples'\n",
    "\n",
    "regexp_tokenize(text, r'\\$?\\d+\\.?\\d*|\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb23c76e-f129-481a-830c-9ec209c96b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : giv\n",
      "giving : giv\n",
      "given : giv\n",
      "gave : gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "words_to_stem = ['give', 'giving', 'given', 'gave']\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words +\" : \"+ lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3c1f1b62-1ed7-49f0-8d01-b6726e01749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "given : given\n",
      "gave : gave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "sbst = SnowballStemmer('english')\n",
    "\n",
    "words_to_stem = ['give', 'giving', 'given', 'gave']\n",
    "for words in words_to_stem:\n",
    "    print(words +\" : \"+ sbst.stem(words))\n",
    "\n",
    "stemmer = SnowballStemmer('german')\n",
    "stemmer.stem(\"Autobahnen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18261274-938f-4f21-ad30-87d55349e581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
